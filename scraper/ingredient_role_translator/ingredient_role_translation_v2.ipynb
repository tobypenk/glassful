{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split();\n",
    "        word = values[0];\n",
    "        coefs = np.asarray(values[1:], dtype='float32');\n",
    "        embeddings_index[word] = coefs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_dataframe_col(df,col):\n",
    "    df[col] = df[col].astype(str).str.lower()\n",
    "    df[col] =  [re.sub(r'\\s+',' ',re.sub(r'\\([^)]*\\)', '', str(x))).strip() for x in df[col]]\n",
    "    return(df)\n",
    "\n",
    "#hyperparameters\n",
    "################################<-training####<-validation####<-testing\n",
    "EPOCHS = 5\n",
    "TEST_PORTION = 0.1\n",
    "VALIDATION_PORTION = 0.1\n",
    "LEARNING_RATE = 0.005\n",
    "SAMPLES = 20000\n",
    "\n",
    "#derived parameters\n",
    "end_testing = int(SAMPLES * (1 - VALIDATION_PORTION))\n",
    "end_training = int(end_testing * (1 - TEST_PORTION))\n",
    "\n",
    "source_col = 'string'\n",
    "translated_col = 'i_cat'\n",
    "\n",
    "base = pd.read_csv('base_strings_v2.csv').sample(SAMPLES)\n",
    "\n",
    "base = clean_dataframe_col(base,source_col)\n",
    "base = clean_dataframe_col(base,translated_col)\n",
    "\n",
    "#base_training = base[:end_training]\n",
    "#base_validation = base[end_training:end_testing]\n",
    "#base_testing = base[end_testing:]\n",
    "\n",
    "ingredient_strings = base[source_col]\n",
    "category_strings = base[translated_col]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# initialize tokenizers and apply to sentences and labels\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(ingredient_strings)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)\n",
    "sequences = tokenizer.texts_to_sequences(ingredient_strings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the embeddings to create the pretrained weights for this corpus\n",
    "embedding_dim = 100\n",
    "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word);\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector;\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(base,trns,tk):\n",
    "    \n",
    "    base = base.split()\n",
    "    trns = trns.split()\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    seq_len = len(base)\n",
    "    \n",
    "    for i in range(seq_len):\n",
    "        tokens.append({\n",
    "            \"metadata\": [\n",
    "                seq_len,\n",
    "                i,\n",
    "                seq_len - i - 1,\n",
    "                False if (i == 0) else base[i-1][len(base[i-1]) - 1] == \",\",\n",
    "                base[i][len(base[i]) - 1] == \",\",\n",
    "                base[i][0].upper() == base[i][0]\n",
    "            ],\n",
    "            \"words\": {\n",
    "                \"m3\": tk.texts_to_sequences('' if (i < 3) else base[i - 3].replace(\",\",\"\")),\n",
    "                \"m2\": '' if (i < 2) else base[i - 2].replace(\",\",\"\"),\n",
    "                \"m1\": '' if (i < 1) else base[i - 1].replace(\",\",\"\"),\n",
    "                \"word\": base[i].replace(\",\",\"\"),\n",
    "                \"f1\": '' if (seq_len - i - 1 < 1) else base[i + 1].replace(\",\",\"\"),\n",
    "                \"f2\": '' if (seq_len - i - 1 < 2) else base[i + 2].replace(\",\",\"\"),\n",
    "                \"f3\": '' if (seq_len - i - 1 < 3) else base[i + 3].replace(\",\",\"\")\n",
    "            },\n",
    "            \"class\": trns[i]\n",
    "        })\n",
    "    return tokens\n",
    "        \n",
    "        #word_dict = {\n",
    "        #    'phrase_length': seq_len,\n",
    "        #    'from_start': i,\n",
    "        #    'from_end': seq_len - i - 1,\n",
    "        #    'comma_precedes': False if (i == 0) else base[i-1][len(base[i-1]) - 1] == \",\",\n",
    "        #    'comma_follows': base[i][len(base[i]) - 1] == \",\",\n",
    "        #    'is_capitalized': base[i][0].upper() == base[i][0],\n",
    "        #    'three_back': '' if (i < 3) else base[i - 3].replace(\",\",\"\"),\n",
    "        #    'two_back': '' if (i < 2) else base[i - 2].replace(\",\",\"\"),\n",
    "        #    'one_back': '' if (i < 1) else base[i - 1].replace(\",\",\"\"),\n",
    "        #    'this_term': base[i].replace(\",\",\"\"),\n",
    "        #    'one_fwd': '' if (seq_len - i - 1 < 1) else base[i + 1].replace(\",\",\"\"),\n",
    "        #    'two_fwd': '' if (seq_len - i - 1 < 2) else base[i + 2].replace(\",\",\"\"),\n",
    "        #    'three_fwd': '' if (seq_len - i - 1 < 3) else base[i + 3].replace(\",\",\"\")\n",
    "        #}\n",
    "        #tokens.append([word_dict,trns[i]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'class': '0',\n",
      "  'metadata': [5, 0, 4, False, False, True],\n",
      "  'words': {'f1': 'tbsp',\n",
      "            'f2': 'kosher',\n",
      "            'f3': 'salt',\n",
      "            'm1': '',\n",
      "            'm2': '',\n",
      "            'm3': [],\n",
      "            'word': '2'}},\n",
      " {'class': '1',\n",
      "  'metadata': [5, 1, 3, False, False, False],\n",
      "  'words': {'f1': 'kosher',\n",
      "            'f2': 'salt',\n",
      "            'f3': 'divided',\n",
      "            'm1': '2',\n",
      "            'm2': '',\n",
      "            'm3': [],\n",
      "            'word': 'tbsp'}},\n",
      " {'class': '3',\n",
      "  'metadata': [5, 2, 2, False, False, False],\n",
      "  'words': {'f1': 'salt',\n",
      "            'f2': 'divided',\n",
      "            'f3': '',\n",
      "            'm1': 'tbsp',\n",
      "            'm2': '2',\n",
      "            'm3': [],\n",
      "            'word': 'kosher'}},\n",
      " {'class': '3',\n",
      "  'metadata': [5, 3, 1, False, True, False],\n",
      "  'words': {'f1': 'divided',\n",
      "            'f2': '',\n",
      "            'f3': '',\n",
      "            'm1': 'kosher',\n",
      "            'm2': 'tbsp',\n",
      "            'm3': [[6]],\n",
      "            'word': 'salt'}},\n",
      " {'class': '4',\n",
      "  'metadata': [5, 4, 0, True, False, False],\n",
      "  'words': {'f1': '',\n",
      "            'f2': '',\n",
      "            'f3': '',\n",
      "            'm1': 'salt',\n",
      "            'm2': 'kosher',\n",
      "            'm3': [[1], [1], [1], [1]],\n",
      "            'word': 'divided'}}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pprint.pprint(get_features('2 tbsp kosher salt, divided','0 1 3 3 4',tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, concatenate, Dense\n",
    "\n",
    "text_in = Input(shape=(embedding_dim,))\n",
    "meta_in = Input(shape=(6,))\n",
    "embedded_text = Embedding(\n",
    "    vocab_size+1, \n",
    "    embedding_dim \n",
    "    #input_length=max_length, \n",
    "    #weights=[embeddings_matrix], \n",
    "    #trainable=False\n",
    ")(text_in)\n",
    "# You process text_in however you like\n",
    "text_features = LSTM(100)(embedded_text)\n",
    "merged = concatenate([text_features, meta_in]) # (samples, 101)\n",
    "text_class = Dense(6, activation='softmax')(merged)\n",
    "model = Model([text_in, meta_in], text_class)\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "#history = model.fit(\n",
    "#    trn, \n",
    "#    trn_lb, \n",
    "#    epochs=10, \n",
    "#    validation_data=(val, val_lb), \n",
    "#    verbose=1\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NLP Course - Week 3 Exercise Answer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
