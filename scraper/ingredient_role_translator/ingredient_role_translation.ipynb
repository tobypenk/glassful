{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import helper\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LEARNING_RATE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7bded99fcd57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mcategory_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0membeddings_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m ):\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LEARNING_RATE' is not defined"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    generates tokenizer for corpus\n",
    "    \n",
    "    inputs:\n",
    "        x: vector of base strings\n",
    "        \n",
    "    outputs: \n",
    "        vector of tokenized string sequences\n",
    "        tokenizer\n",
    "    \"\"\"\n",
    "    x_tk = Tokenizer(\n",
    "        char_level = False,\n",
    "        filters='!\"#$%&()*+.:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "        oov_token=\"<OOV>\"\n",
    "    )\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "\n",
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    adds padding tokens to the end of strings to make them conform to \n",
    "        a uniform length.\n",
    "    \n",
    "    inputs:\n",
    "        x: a vector of token sequences\n",
    "        length: the intended length to which to pad each sequence\n",
    "        \n",
    "    outputs:\n",
    "        vector of padded sequences\n",
    "    \"\"\"\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen = length, padding = 'post')\n",
    "\n",
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    cleans, tokenizes, and pads input strings\n",
    "    \n",
    "    inputs: \n",
    "        x: vector of base strings\n",
    "        y: vector of translated strings\n",
    "        \n",
    "    outputs:\n",
    "        tokenized, padded base strings\n",
    "        tokenized, padded translated strings\n",
    "        base string tokenizer\n",
    "        translated string tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x.astype(str).str.lower()\n",
    "    x = [re.sub(r'\\s+',' ',re.sub(r'\\([^)]*\\)', '', str(z))).strip() for z in x]\n",
    "    x = [re.sub(r\"([0-9]+(\\.[0-9]+)?)\",r\" \\1 \", str(z)).strip() for z in x]\n",
    "    x = [z.replace(\"-\",\" \") for z in x]\n",
    "    \n",
    "    y = y.astype(str).str.lower()\n",
    "    y = [re.sub(r'\\s+',' ',re.sub(r'\\([^)]*\\)', '', str(z))).strip() for z in y]\n",
    "    y = [re.sub(r\"([0-9]+(\\.[0-9]+)?)\",r\" \\1 \", str(z)).strip() for z in y]\n",
    "    y = [z.replace(\"-\",\" \") for z in y]\n",
    "    \n",
    "    # outputs tokenized sequences (1,2) and tokenizers (3,4)\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "    \n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "    \n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_v2(x, y):\n",
    "    \"\"\"\n",
    "    cleans, tokenizes, and pads input strings\n",
    "    \n",
    "    inputs: \n",
    "        x: vector of base strings\n",
    "        y: vector of translated strings\n",
    "        \n",
    "    outputs:\n",
    "        tokenized, padded base strings\n",
    "        tokenized, padded translated strings\n",
    "        base string tokenizer\n",
    "        translated string tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x.astype(str).str.lower()\n",
    "    x = [re.sub(r'\\s+',' ',re.sub(r'\\([^)]*\\)', '', str(z))).strip() for z in x]\n",
    "    x = [re.sub(r\"([0-9]+(\\.[0-9]+)?)\",r\" \\1 \", str(z)).strip() for z in x]\n",
    "    x = [z.replace(\"-\",\" \") for z in x]\n",
    "\n",
    "    y = y.astype(str).str.lower()\n",
    "    y = [re.sub(r'\\s+',' ',re.sub(r'\\([^)]*\\)', '', str(z))).strip() for z in y]\n",
    "    y = [re.sub(r\"([0-9]+(\\.[0-9]+)?)\",r\" \\1 \", str(z)).strip() for z in y]\n",
    "    y = [z.replace(\"-\",\" \") for z in y]\n",
    "    \n",
    "    all_strings = ingredient_strings\n",
    "    all_strings.append(category_strings)\n",
    "    \n",
    "    all_tokenizer = Tokenizer(\n",
    "        char_level = False,\n",
    "        filters='!\"#$%&()*+.:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "        oov_token=\"<OOV>\"\n",
    "    )\n",
    "    all_tokenizer.fit_on_texts(all_strings)\n",
    "    #return x_tk.texts_to_sequences(x), x_tk\n",
    "    \n",
    "    # outputs tokenized sequences (1,2) and tokenizers (3,4)\n",
    "    preprocess_x = all_tokenizer.texts_to_sequences(x)\n",
    "    preprocess_y = all_tokenizer.texts_to_sequences(y)\n",
    "    \n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "    \n",
    "    return preprocess_x, preprocess_y, all_tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    turns numeric predictions back into readable text\n",
    "    \n",
    "    inputs:\n",
    "        logits: the output prediction of the trained model\n",
    "        tokenizer: the tokenizer used to encode the predicted sequence\n",
    "        \n",
    "    outputs:\n",
    "        a string representing the translated phrase\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "def model_final(\n",
    "    input_shape,\n",
    "    output_sequence_length,\n",
    "    ingredient_vocab_size,\n",
    "    category_vocab_size,\n",
    "    embeddings_matrix,\n",
    "    learning_rate = LEARNING_RATE\n",
    "):\n",
    "  \n",
    "    model = Sequential()\n",
    "    #model.add(Embedding(\n",
    "    #    input_dim=ingredient_vocab_size,\n",
    "    #    output_dim=128,\n",
    "    #    input_length=input_shape[1]\n",
    "    #))\n",
    "    model.add(Embedding(\n",
    "        ingredient_vocab_size, \n",
    "        embedding_dim, \n",
    "        input_length=input_shape[1], \n",
    "        weights=[embeddings_matrix], \n",
    "        trainable=False\n",
    "    ))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(GRU(256,return_sequences=False)))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(Bidirectional(GRU(256,return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(category_vocab_size,activation='softmax')))\n",
    "    \n",
    "    model.compile(\n",
    "        loss = sparse_categorical_crossentropy,\n",
    "        optimizer = Adam(learning_rate),\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_prediction(x, y, x_tk, y_tk, i, m):\n",
    "    \n",
    "    #need to try tokenizer.texts_to_sequences\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "    \n",
    "    i = [x_tk.word_index[word] for word in i.split()]\n",
    "    #i = x_tk.texts_to_sequences(i)\n",
    "    i = pad_sequences([i], maxlen=x.shape[-1], padding='post')\n",
    "    i = np.array([i[0], x[0]])\n",
    "    predictions = m.predict(i, len(i))\n",
    "\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "################################<-training####<-validation####<-testing\n",
    "EPOCHS = 9\n",
    "TEST_PORTION = 0.1\n",
    "VALIDATION_PORTION = 0.1\n",
    "LEARNING_RATE = 0.005\n",
    "SAMPLES = 222300\n",
    "\n",
    "#derived parameters\n",
    "end_testing = int(SAMPLES * (1 - VALIDATION_PORTION))\n",
    "end_training = int(end_testing * (1 - TEST_PORTION))\n",
    "\n",
    "base = pd.read_csv(\n",
    "    'part_tagging_data/translation_corpus_v2.csv').sample(SAMPLES)\n",
    "base.i_str = base.i_str.astype(str)\n",
    "base.string = base.string.astype(str)\n",
    "\n",
    "base_training = base[:end_training]\n",
    "base_validation = base[end_training:end_testing]\n",
    "base_testing = base[end_testing:]\n",
    "\n",
    "ingredient_strings = base.string\n",
    "category_strings = base.i_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_ingredient_strings,\\\n",
    "preproc_category_strings,\\\n",
    "all_tokenizer =\\\n",
    "preprocess_v2(ingredient_strings, category_strings)\n",
    "\n",
    "ingredient_training = preproc_ingredient_strings[:end_training]\n",
    "ingredient_validation = preproc_ingredient_strings[end_training:end_testing]\n",
    "ingredient_testing = preproc_ingredient_strings[end_testing:]\n",
    "\n",
    "category_training = preproc_category_strings[:end_training]\n",
    "category_validation = preproc_category_strings[end_training:end_testing]\n",
    "category_testing = preproc_category_strings[end_testing:]\n",
    "\n",
    "max_ingredient_sequence_length = preproc_ingredient_strings.shape[1]\n",
    "max_category_sequence_length = preproc_category_strings.shape[1]\n",
    "ingredient_vocab_size = len(ingredient_tokenizer.word_index)\n",
    "category_vocab_size = len(category_tokenizer.word_index) + 1\n",
    "\n",
    "tmp_x = pad(preproc_ingredient_strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the embeddings matrix\n",
    "embedding_dim = 100\n",
    "\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split();\n",
    "        word = values[0];\n",
    "        coefs = np.asarray(values[1:], dtype='float32');\n",
    "        embeddings_index[word] = coefs;\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the embeddings to create the pretrained weights for this corpus\n",
    "embeddings_matrix = np.zeros((len(all_tokenizer.word_index)+1, embedding_dim));\n",
    "for word, i in all_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word);\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector;\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f = model_final(\n",
    "    tmp_x.shape,\n",
    "    preproc_category_strings.shape[1],\n",
    "    len(all_tokenizer.word_index)+1,\n",
    "    len(all_tokenizer.word_index)+1,\n",
    "    embeddings_matrix\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "869/869 [==============================] - 451s 519ms/step - loss: 0.8110 - accuracy: 0.8630 - val_loss: 0.2158 - val_accuracy: 0.9572\n",
      "Epoch 2/9\n",
      "869/869 [==============================] - 423s 487ms/step - loss: 0.2118 - accuracy: 0.9576 - val_loss: 0.1759 - val_accuracy: 0.9639\n",
      "Epoch 3/9\n",
      "869/869 [==============================] - 417s 480ms/step - loss: 0.1913 - accuracy: 0.9611 - val_loss: 0.1760 - val_accuracy: 0.9635\n",
      "Epoch 4/9\n",
      "869/869 [==============================] - 420s 484ms/step - loss: 0.1840 - accuracy: 0.9620 - val_loss: 0.1697 - val_accuracy: 0.9645\n",
      "Epoch 5/9\n",
      "869/869 [==============================] - 416s 479ms/step - loss: 0.1860 - accuracy: 0.9610 - val_loss: 0.1699 - val_accuracy: 0.9641\n",
      "Epoch 6/9\n",
      "869/869 [==============================] - 418s 481ms/step - loss: 0.1781 - accuracy: 0.9624 - val_loss: 0.1637 - val_accuracy: 0.9648\n",
      "Epoch 7/9\n",
      "869/869 [==============================] - 415s 478ms/step - loss: 0.1749 - accuracy: 0.9628 - val_loss: 0.1622 - val_accuracy: 0.9654\n",
      "Epoch 8/9\n",
      "869/869 [==============================] - 448s 516ms/step - loss: 0.1752 - accuracy: 0.9625 - val_loss: 0.1610 - val_accuracy: 0.9656\n",
      "Epoch 9/9\n",
      "869/869 [==============================] - 444s 511ms/step - loss: 0.1714 - accuracy: 0.9631 - val_loss: 0.1601 - val_accuracy: 0.9653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f90f525f250>"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_f.fit(\n",
    "    tmp_x, \n",
    "    preproc_category_strings, \n",
    "    batch_size = 256, \n",
    "    epochs = EPOCHS, \n",
    "    validation_data = (ingredient_validation,category_validation)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv(\"part_tagging_data/all_df_reconciler_v2.csv\")\n",
    "new_targets = all_df.string.astype(str)\n",
    "#print(new_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(x, y, x_tk, y_tk, i, m):\n",
    "    \n",
    "    \n",
    "    #need to try tokenizer.texts_to_sequences\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "    \n",
    "    #i = [x_tk.word_index[word] for word in i.split()]\n",
    "    \n",
    "    i = i.astype(str).str.lower()\n",
    "    i = [re.sub(r'\\s+',' ',re.sub(r'\\([^)]*\\)', '', str(z))).strip() for z in i]\n",
    "    i = [re.sub(r\"([0-9]+(\\.[0-9]+)?)\",r\" \\1 \", str(z)).strip() for z in i]\n",
    "    i = [z.replace(\"-\",\" \") for z in i]\n",
    "    \n",
    "    i = x_tk.texts_to_sequences(i)\n",
    "    #i = x_tk.texts_to_sequences(i)\n",
    "    i = pad_sequences(i, maxlen=x.shape[-1], padding='post')\n",
    "    i = np.array(i)\n",
    "    #predictions = m.predict(i)\n",
    "    predictions = list(map(\n",
    "        lambda x: ' '.join(\n",
    "            filter(lambda z: z != \"<PAD>\",[y_id_to_word[np.argmax(y)] for y in x])\n",
    "        ),\n",
    "        m.predict(i)\n",
    "    ))\n",
    "    return predictions\n",
    "    #print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "    \n",
    "    \n",
    "testing_preds = get_prediction(\n",
    "    preproc_ingredient_strings,\n",
    "    preproc_category_strings, \n",
    "    all_tokenizer, \n",
    "    all_tokenizer,\n",
    "    new_targets,\n",
    "    model_f\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f.save('ingredient_role_translator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['preds'] = testing_preds\n",
    "all_df.to_csv(\"part_tagging_data/all_df_reconciler_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ripe chocolate', 'the kings juice', 'milk']\n"
     ]
    }
   ],
   "source": [
    "test_ings = pd.DataFrame([\n",
    "    \"'1  cup frosting canned or from scratch; try vanilla or cream cheese or chocolate\",\n",
    "    \"2   4  ounces vodka depending on how strong you want it\",\n",
    "    \"Sprinkles and corn syrup or honey for rimming the glasses\"\n",
    "])\n",
    "print(\n",
    "    get_prediction(\n",
    "    preproc_ingredient_strings,\n",
    "    preproc_category_strings, \n",
    "    all_tokenizer, \n",
    "    all_tokenizer,\n",
    "    test_ings[0].astype(str),\n",
    "    model_f\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### to delete i think\n",
    "\n",
    "\n",
    "\n",
    "#base_testing['preds'] = testing_preds\n",
    "print(base_testing.head())\n",
    "base_testing.to_csv(\"reconciler.csv\")\n",
    "\n",
    "\n",
    "def final_predictions(x, y, x_tk, y_tk, m, tst_snt):\n",
    "    tmp_X = pad(preproc_ingredient_strings)\n",
    "\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "    \n",
    "    tst_snt = [x_tk.word_index[word] for word in tst_snt.split()]\n",
    "    \n",
    "    tst_snt = pad_sequences([tst_snt], maxlen=x.shape[-1], padding='post')\n",
    "    tst_snt = pad(tst_snt, y.shape[1])\n",
    "    predictions = m.predict(tst_snt.reshape((-1, y.shape[-2], 1)))\n",
    "\n",
    "    print('Sample 1:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "\n",
    "#final_predictions(\n",
    "#    preproc_ingredient_strings, \n",
    "#    preproc_category_strings, \n",
    "#    ingredient_tokenizer, \n",
    "#    category_tokenizer,\n",
    "#    bidi_model,\n",
    "#    \"2 3/4 oz quality soy sauce, warmed\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NLP Course - Week 3 Exercise Answer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
